<!doctype html>
<html lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.1">
<link rel="alternate" type="application/rss+xml" href="/mobile-ds/blog/rss.xml" title="PyTorch Mobile Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/mobile-ds/blog/atom.xml" title="PyTorch Mobile Blog Atom Feed"><title data-react-helmet="true">Quantization | PyTorch Mobile</title><meta data-react-helmet="true" property="og:url" content="https://brianjo.github.com/mobile-ds/docs/modelprep/quantization"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="Quantization | PyTorch Mobile"><meta data-react-helmet="true" name="description" content="This recipe demonstrates how to quantize a PyTorch model so it can run"><meta data-react-helmet="true" property="og:description" content="This recipe demonstrates how to quantize a PyTorch model so it can run"><link data-react-helmet="true" rel="shortcut icon" href="/mobile-ds/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://brianjo.github.com/mobile-ds/docs/modelprep/quantization"><link data-react-helmet="true" rel="alternate" href="https://brianjo.github.com/mobile-ds/docs/modelprep/quantization" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://brianjo.github.com/mobile-ds/docs/modelprep/quantization" hreflang="x-default"><link rel="stylesheet" href="/mobile-ds/assets/css/styles.f84ef0c2.css">
<link rel="preload" href="/mobile-ds/assets/js/runtime~main.e05b8f85.js" as="script">
<link rel="preload" href="/mobile-ds/assets/js/main.b7e865d3.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div><a href="#main" class="skipToContent_1oUP shadow--md">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/mobile-ds/"><img src="/mobile-ds/img/logo.svg" alt="PyTorch" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/mobile-ds/img/logo.svg" alt="PyTorch" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">PyTorch Mobile</b></a><a class="navbar__item navbar__link navbar__link--active" href="/mobile-ds/docs/intro">Documentation</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/mobile-ds/"><img src="/mobile-ds/img/logo.svg" alt="PyTorch" class="themedImage_1VuW themedImage--light_3UqQ navbar__logo"><img src="/mobile-ds/img/logo.svg" alt="PyTorch" class="themedImage_1VuW themedImage--dark_hz6m navbar__logo"><b class="navbar__title">PyTorch Mobile</b></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a class="menu__link navbar__link--active" href="/mobile-ds/docs/intro">Documentation</a></li><li class="menu__list-item"><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer" class="menu__link"><span>GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_3J9K"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></span></a></li></ul></div></div></div></nav><div class="main-wrapper docs-wrapper doc-page"><div class="docPage_31aa"><aside class="docSidebarContainer_3Kbt"><div class="sidebar_15mo"><nav class="menu menu--responsive thin-scrollbar menu_Bmed" aria-label="Sidebar navigation"><button aria-label="Open menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg class="sidebarMenuIcon_fgN0" width="24" height="24" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/mobile-ds/docs/intro">PyTorch Mobile</a></li><li class="menu__list-item"><a class="menu__link" href="/mobile-ds/docs/helloworld">Hello World!</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Model Preparation</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/mobile-ds/docs/modelprep/interpreters">Lite and Full Jit Interpreters</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/mobile-ds/docs/modelprep/optimization">Script and Optimize for Mobile</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/mobile-ds/docs/modelprep/quantization">Quantization</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Build from Source</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/mobile-ds/docs/building/androidbuild">Building PyTorch Android from Source</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/mobile-ds/docs/building/iosbuild">Build PyTorch iOS Libraries from Source</a></li></ul></li><li class="menu__list-item"><a class="menu__link" href="/mobile-ds/docs/android">Android</a></li><li class="menu__list-item"><a class="menu__link" href="/mobile-ds/docs/ios">iOS</a></li></ul></nav></div></aside><main class="docMainContainer_3ufF"><div class="container padding-top--md padding-bottom--lg docItemWrapper_3FMP"><div class="row"><div class="col docItemCol_3FnS"><div class="docItemContainer_33ec"><article><div class="markdown"><header><h1 class="h1Heading_27L5">Quantization</h1></header><p>This recipe demonstrates how to quantize a PyTorch model so it can run
with reduced size and faster inference speed with about the same
accuracy as the original model. Quantization can be applied to both
server and mobile model deployment, but it can be especially important
or even critical on mobile, because a non-quantized model&#x27;s size may
exceed the limit that an iOS or Android app allows for, cause the
deployment or OTA update to take too much time, and make the inference
too slow for a good user experience.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h2><p>Quantization is a technique that converts 32-bit floating numbers in the
model parameters to 8-bit integers. With quantization, the model size
and memory footprint can be reduced to 1/4 of its original size, and the
inference can be made about 2-4 times faster, while the accuracy stays
about the same.</p><p>There are overall three approaches or workflows to quantize a model:
post training dynamic quantization, post training static quantization,
and quantization aware training. But if the model you want to use
already has a quantized version, you can use it directly without going
through any of the three workflows above. For example, the <span class="title-ref">torchvision</span> library already includes quantized
versions for models MobileNet v2, ResNet 18, ResNet 50, Inception v3,
GoogleNet, among others. So we will make the last approach another
workflow, albeit a simple one.</p><p>Note</p><p>The quantization support is available for a limited set of operators.
See
<a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#device-and-operator-support" target="_blank" rel="noopener noreferrer">this</a>
for more information.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="pre-requisites"></a>Pre-requisites<a class="hash-link" href="#pre-requisites" title="Direct link to heading">#</a></h2><p>PyTorch 1.6.0 or 1.7.0</p><p>torchvision 0.6.0 or 0.7.0</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="workflows"></a>Workflows<a class="hash-link" href="#workflows" title="Direct link to heading">#</a></h2><p>Use one of the four workflows below to quantize a model.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="1-use-pretrained-quantized-mobilenet-v2"></a>1. Use Pretrained Quantized MobileNet v2<a class="hash-link" href="#1-use-pretrained-quantized-mobilenet-v2" title="Direct link to heading">#</a></h3><p>To get the MobileNet v2 quantized model, simply do:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">import torchvision</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>To compare the size difference of a non-quantized MobileNet v2 model
with its quantized version:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">model = torchvision.models.mobilenet_v2(pretrained=True)</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">import os</span></span><span class="token-line" style="color:#393A34"><span class="token plain">import torch</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">def print_model_size(mdl):</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    torch.save(mdl.state_dict(), &quot;tmp.pt&quot;)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    print(&quot;%.2f MB&quot; %(os.path.getsize(&quot;tmp.pt&quot;)/1e6))</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    os.remove(&#x27;tmp.pt&#x27;)</span></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block">
</span></span><span class="token-line" style="color:#393A34"><span class="token plain">print_model_size(model)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">print_model_size(model_quantized)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>The outputs will be:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">14.27 MB</span></span><span class="token-line" style="color:#393A34"><span class="token plain">3.63 MB</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="2-post-training-dynamic-quantization"></a>2. Post Training Dynamic Quantization<a class="hash-link" href="#2-post-training-dynamic-quantization" title="Direct link to heading">#</a></h3><p>To apply Dynamic Quantization, which converts all the weights in a model
from 32-bit floating numbers to 8-bit integers but doesn&#x27;t convert the
activations to int8 till just before performing the computation on the
activations, simply call `torch.quantization.quantize_dynamic`:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">model_dynamic_quantized = torch.quantization.quantize_dynamic(</span></span><span class="token-line" style="color:#393A34"><span class="token plain">    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8</span></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>where <span class="title-ref">qconfig_spec</span> specifies the list of
submodule names in <span class="title-ref">model</span> to apply
quantization to.</p><p>Warning</p><p>An important limitation of Dynamic Quantization, while it is the easiest
workflow if you do not have a pre-trained quantized model ready for use,
is that it currently only supports <span class="title-ref">nn.Linear</span> and <span class="title-ref">nn.LSTM</span> in <span class="title-ref">qconfig_spec</span>, meaning that you will have to
use Static Quantization or Quantization Aware Training, to be discussed
later, to quantize other modules such as <span class="title-ref">nn.Conv2d</span>.</p><p>The full documentation of the <span class="title-ref">quantize_dynamic</span> API call is
<a href="https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic" target="_blank" rel="noopener noreferrer">here</a>.
Three other examples of using the post training dynamic quantization are
<a href="https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html" target="_blank" rel="noopener noreferrer">the Bert
example</a>,
<a href="https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization" target="_blank" rel="noopener noreferrer">an LSTM model
example</a>,
and another <a href="https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-the-quantization" target="_blank" rel="noopener noreferrer">demo LSTM
example</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="3-post-training-static-quantization"></a>3. Post Training Static Quantization<a class="hash-link" href="#3-post-training-static-quantization" title="Direct link to heading">#</a></h3><p>This method converts both the weights and the activations to 8-bit
integers beforehand so there won&#x27;t be on-the-fly conversion on the
activations during the inference, as the dynamic quantization does,
hence improving the performance significantly.</p><p>To apply static quantization on a model, run the following code:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">backend = &quot;qnnpack&quot;</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model.qconfig = torch.quantization.get_default_qconfig(backend)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">torch.backends.quantized.engine = backend</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model_static_quantized = torch.quantization.prepare(model, inplace=False)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>After this, running <span class="title-ref">print_model_size(model_static_quantized)</span>
shows the static quantized model is <span class="title-ref">3.98MB</span>.</p><p>A complete model definition and static quantization example is
<a href="https://pytorch.org/docs/stable/quantization.html#quantization-api-summary" target="_blank" rel="noopener noreferrer">here</a>.
A dedicated static quantization tutorial is
<a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html" target="_blank" rel="noopener noreferrer">here</a>.</p><p>Note</p><p>To make the model run on mobile devices which normally have arm
architecture, you need to use <span class="title-ref">qnnpack</span> for</p><span class="title-ref">backend</span>; to run the model on computer with x86 architecture, use <span class="title-ref">fbgemm</span>.<h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="4-quantization-aware-training"></a>4. Quantization Aware Training<a class="hash-link" href="#4-quantization-aware-training" title="Direct link to heading">#</a></h3><p>Quantization aware training inserts fake quantization to all the weights
and activations during the model training process and results in higher
inference accuracy than the post-training quantization methods. It is
typically used in CNN models.</p><p>To enable a model for quantization aware traing, define in the <span class="title-ref">__init__</span> method of the model definition a</p><span class="title-ref">QuantStub</span> and a <span class="title-ref">DeQuantStub</span> to convert tensors from floating point to quantized type and vice versa:<div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">self.quant = torch.quantization.QuantStub()</span></span><span class="token-line" style="color:#393A34"><span class="token plain">self.dequant = torch.quantization.DeQuantStub()</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>Then in the beginning and the end of the <span class="title-ref">forward</span> method of the model definition, call</p><span class="title-ref">x = self.quant(x)</span> and <span class="title-ref">x = self.dequant(x)</span>.<p>To do a quantization aware training, use the following code snippet:</p><div class="codeBlockContainer_K1bP"><div class="codeBlockContent_hGly"><pre tabindex="0" class="prism-code language-undefined codeBlock_23N8 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_39YC"><span class="token-line" style="color:#393A34"><span class="token plain">model.qconfig = torch.quantization.get_default_qat_qconfig(backend)</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model_qat = torch.quantization.prepare_qat(model, inplace=False)</span></span><span class="token-line" style="color:#393A34"><span class="token plain"># quantization aware training goes here</span></span><span class="token-line" style="color:#393A34"><span class="token plain">model_qat = torch.quantization.convert(model_qat.eval(), inplace=False)</span></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_Ue-o clean-btn">Copy</button></div></div><p>For more detailed examples of the quantization aware training, see
<a href="https://pytorch.org/docs/master/quantization.html#quantization-aware-training" target="_blank" rel="noopener noreferrer">here</a>
and
<a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training" target="_blank" rel="noopener noreferrer">here</a>.</p><p>A pre-trained quantized model can also be used for quantized aware
transfer learning, using the same <span class="title-ref">quant</span>
and <span class="title-ref">dequant</span> calls shown above. See
<a href="https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor" target="_blank" rel="noopener noreferrer">here</a>
for a complete example.</p><p>After a quantized model is generated using one of the steps above,
before the model can be used to run on mobile devices, it needs to be
further converted to the <span class="title-ref">TorchScript</span>
format and then optimized for mobile apps. See the <a href="/mobile-ds/docs/modelprep/script_optimized.html">Script and Optimize
for Mobile recipe</a> for details.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2LWZ" id="learn-more"></a>Learn More<a class="hash-link" href="#learn-more" title="Direct link to heading">#</a></h2><p>For more info on the different workflows of quantization, see
<a href="https://pytorch.org/docs/stable/quantization.html#quantization-workflows" target="_blank" rel="noopener noreferrer">here</a>
and
<a href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#post-training-static-quantization" target="_blank" rel="noopener noreferrer">here</a>.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><a href="https://github.com/facebook/docusaurus/edit/master/website/docs/modelprep/quantization.md" target="_blank" rel="noreferrer noopener"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_2_ui" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_3DPF"></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/mobile-ds/docs/modelprep/optimization"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« Script and Optimize for Mobile</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/mobile-ds/docs/building/androidbuild"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Building PyTorch Android from Source »</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_35-E thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link">Introduction</a></li><li><a href="#pre-requisites" class="table-of-contents__link">Pre-requisites</a></li><li><a href="#workflows" class="table-of-contents__link">Workflows</a><ul><li><a href="#1-use-pretrained-quantized-mobilenet-v2" class="table-of-contents__link">1. Use Pretrained Quantized MobileNet v2</a></li><li><a href="#2-post-training-dynamic-quantization" class="table-of-contents__link">2. Post Training Dynamic Quantization</a></li><li><a href="#3-post-training-static-quantization" class="table-of-contents__link">3. Post Training Static Quantization</a></li><li><a href="#4-quantization-aware-training" class="table-of-contents__link">4. Quantization Aware Training</a></li></ul></li><li><a href="#learn-more" class="table-of-contents__link">Learn More</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/mobile-ds/docs/intro">Documentation</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow</a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord</a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter</a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2021 My Project, Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/mobile-ds/assets/js/runtime~main.e05b8f85.js"></script>
<script src="/mobile-ds/assets/js/main.b7e865d3.js"></script>
</body>
</html>