(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[757],{3905:function(t,e,n){"use strict";n.d(e,{Zo:function(){return c},kt:function(){return m}});var a=n(7294);function i(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function o(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,a)}return n}function r(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?o(Object(n),!0).forEach((function(e){i(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function l(t,e){if(null==t)return{};var n,a,i=function(t,e){if(null==t)return{};var n,a,i={},o=Object.keys(t);for(a=0;a<o.length;a++)n=o[a],e.indexOf(n)>=0||(i[n]=t[n]);return i}(t,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(t);for(a=0;a<o.length;a++)n=o[a],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(i[n]=t[n])}return i}var s=a.createContext({}),u=function(t){var e=a.useContext(s),n=e;return t&&(n="function"==typeof t?t(e):r(r({},e),t)),n},c=function(t){var e=u(t.components);return a.createElement(s.Provider,{value:e},t.children)},p={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},d=a.forwardRef((function(t,e){var n=t.components,i=t.mdxType,o=t.originalType,s=t.parentName,c=l(t,["components","mdxType","originalType","parentName"]),d=u(n),m=i,h=d["".concat(s,".").concat(m)]||d[m]||p[m]||o;return n?a.createElement(h,r(r({ref:e},c),{},{components:n})):a.createElement(h,r({ref:e},c))}));function m(t,e){var n=arguments,i=e&&e.mdxType;if("string"==typeof t||i){var o=n.length,r=new Array(o);r[0]=d;var l={};for(var s in e)hasOwnProperty.call(e,s)&&(l[s]=e[s]);l.originalType=t,l.mdxType="string"==typeof t?t:i,r[1]=l;for(var u=2;u<o;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},5477:function(t,e,n){"use strict";n.r(e),n.d(e,{frontMatter:function(){return l},contentTitle:function(){return s},metadata:function(){return u},toc:function(){return c},default:function(){return d}});var a=n(2122),i=n(9756),o=(n(7294),n(3905)),r=["components"],l={},s="Quantization",u={unversionedId:"modelprep/quantization",id:"modelprep/quantization",isDocsHomePage:!1,title:"Quantization",description:"This recipe demonstrates how to quantize a PyTorch model so it can run",source:"@site/docs/modelprep/quantization.md",sourceDirName:"modelprep",slug:"/modelprep/quantization",permalink:"/docs/modelprep/quantization",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/docs/modelprep/quantization.md",version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Script and Optimize for Mobile",permalink:"/docs/modelprep/optimization"},next:{title:"Building PyTorch Android from Source",permalink:"/docs/building/androidbuild"}},c=[{value:"Introduction",id:"introduction",children:[]},{value:"Pre-requisites",id:"pre-requisites",children:[]},{value:"Workflows",id:"workflows",children:[{value:"1. Use Pretrained Quantized MobileNet v2",id:"1-use-pretrained-quantized-mobilenet-v2",children:[]},{value:"2. Post Training Dynamic Quantization",id:"2-post-training-dynamic-quantization",children:[]},{value:"3. Post Training Static Quantization",id:"3-post-training-static-quantization",children:[]},{value:"4. Quantization Aware Training",id:"4-quantization-aware-training",children:[]}]},{value:"Learn More",id:"learn-more",children:[]}],p={toc:c};function d(t){var e=t.components,n=(0,i.Z)(t,r);return(0,o.kt)("wrapper",(0,a.Z)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"quantization"},"Quantization"),(0,o.kt)("p",null,"This recipe demonstrates how to quantize a PyTorch model so it can run\nwith reduced size and faster inference speed with about the same\naccuracy as the original model. Quantization can be applied to both\nserver and mobile model deployment, but it can be especially important\nor even critical on mobile, because a non-quantized model's size may\nexceed the limit that an iOS or Android app allows for, cause the\ndeployment or OTA update to take too much time, and make the inference\ntoo slow for a good user experience."),(0,o.kt)("h2",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"Quantization is a technique that converts 32-bit floating numbers in the\nmodel parameters to 8-bit integers. With quantization, the model size\nand memory footprint can be reduced to 1/4 of its original size, and the\ninference can be made about 2-4 times faster, while the accuracy stays\nabout the same."),(0,o.kt)("p",null,"There are overall three approaches or workflows to quantize a model:\npost training dynamic quantization, post training static quantization,\nand quantization aware training. But if the model you want to use\nalready has a quantized version, you can use it directly without going\nthrough any of the three workflows above. For example, the ",(0,o.kt)("span",{class:"title-ref"},"torchvision")," library already includes quantized\nversions for models MobileNet v2, ResNet 18, ResNet 50, Inception v3,\nGoogleNet, among others. So we will make the last approach another\nworkflow, albeit a simple one."),(0,o.kt)("p",null,"Note"),(0,o.kt)("p",null,"The quantization support is available for a limited set of operators.\nSee\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#device-and-operator-support"},"this"),"\nfor more information."),(0,o.kt)("h2",{id:"pre-requisites"},"Pre-requisites"),(0,o.kt)("p",null,"PyTorch 1.6.0 or 1.7.0"),(0,o.kt)("p",null,"torchvision 0.6.0 or 0.7.0"),(0,o.kt)("h2",{id:"workflows"},"Workflows"),(0,o.kt)("p",null,"Use one of the four workflows below to quantize a model."),(0,o.kt)("h3",{id:"1-use-pretrained-quantized-mobilenet-v2"},"1. Use Pretrained Quantized MobileNet v2"),(0,o.kt)("p",null,"To get the MobileNet v2 quantized model, simply do:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"import torchvision\nmodel_quantized = torchvision.models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n")),(0,o.kt)("p",null,"To compare the size difference of a non-quantized MobileNet v2 model\nwith its quantized version:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'model = torchvision.models.mobilenet_v2(pretrained=True)\n\nimport os\nimport torch\n\ndef print_model_size(mdl):\n    torch.save(mdl.state_dict(), "tmp.pt")\n    print("%.2f MB" %(os.path.getsize("tmp.pt")/1e6))\n    os.remove(\'tmp.pt\')\n\nprint_model_size(model)\nprint_model_size(model_quantized)\n')),(0,o.kt)("p",null,"The outputs will be:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"14.27 MB\n3.63 MB\n")),(0,o.kt)("h3",{id:"2-post-training-dynamic-quantization"},"2. Post Training Dynamic Quantization"),(0,o.kt)("p",null,"To apply Dynamic Quantization, which converts all the weights in a model\nfrom 32-bit floating numbers to 8-bit integers but doesn't convert the\nactivations to int8 till just before performing the computation on the\nactivations, simply call ","`","torch.quantization.quantize","_","dynamic","`",":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"model_dynamic_quantized = torch.quantization.quantize_dynamic(\n    model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8\n)\n")),(0,o.kt)("p",null,"where ",(0,o.kt)("span",{class:"title-ref"},"qconfig","_","spec")," specifies the list of\nsubmodule names in ",(0,o.kt)("span",{class:"title-ref"},"model")," to apply\nquantization to."),(0,o.kt)("p",null,"Warning"),(0,o.kt)("p",null,"An important limitation of Dynamic Quantization, while it is the easiest\nworkflow if you do not have a pre-trained quantized model ready for use,\nis that it currently only supports ",(0,o.kt)("span",{class:"title-ref"},"nn.Linear")," and ",(0,o.kt)("span",{class:"title-ref"},"nn.LSTM")," in ",(0,o.kt)("span",{class:"title-ref"},"qconfig","_","spec"),", meaning that you will have to\nuse Static Quantization or Quantization Aware Training, to be discussed\nlater, to quantize other modules such as ",(0,o.kt)("span",{class:"title-ref"},"nn.Conv2d"),"."),(0,o.kt)("p",null,"The full documentation of the ",(0,o.kt)("span",{class:"title-ref"},"quantize","_","dynamic")," API call is\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic"},"here"),".\nThree other examples of using the post training dynamic quantization are\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html"},"the Bert\nexample"),",\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html#test-dynamic-quantization"},"an LSTM model\nexample"),",\nand another ",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html#do-the-quantization"},"demo LSTM\nexample"),"."),(0,o.kt)("h3",{id:"3-post-training-static-quantization"},"3. Post Training Static Quantization"),(0,o.kt)("p",null,"This method converts both the weights and the activations to 8-bit\nintegers beforehand so there won't be on-the-fly conversion on the\nactivations during the inference, as the dynamic quantization does,\nhence improving the performance significantly."),(0,o.kt)("p",null,"To apply static quantization on a model, run the following code:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},'backend = "qnnpack"\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\nmodel_static_quantized = torch.quantization.prepare(model, inplace=False)\nmodel_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)\n')),(0,o.kt)("p",null,"After this, running ",(0,o.kt)("span",{class:"title-ref"},"print","_","model","_","size(model","_","static","_","quantized)"),"\nshows the static quantized model is ",(0,o.kt)("span",{class:"title-ref"},"3.98MB"),"."),(0,o.kt)("p",null,"A complete model definition and static quantization example is\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/quantization.html#quantization-api-summary"},"here"),".\nA dedicated static quantization tutorial is\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html"},"here"),"."),(0,o.kt)("p",null,"Note"),(0,o.kt)("p",null,"To make the model run on mobile devices which normally have arm\narchitecture, you need to use ",(0,o.kt)("span",{class:"title-ref"},"qnnpack")," for"),(0,o.kt)("span",{class:"title-ref"},"backend"),"; to run the model on computer with x86 architecture, use ",(0,o.kt)("span",{class:"title-ref"},"fbgemm"),".",(0,o.kt)("h3",{id:"4-quantization-aware-training"},"4. Quantization Aware Training"),(0,o.kt)("p",null,"Quantization aware training inserts fake quantization to all the weights\nand activations during the model training process and results in higher\ninference accuracy than the post-training quantization methods. It is\ntypically used in CNN models."),(0,o.kt)("p",null,"To enable a model for quantization aware traing, define in the ",(0,o.kt)("span",{class:"title-ref"},"_","_","init","_","_")," method of the model definition a"),(0,o.kt)("span",{class:"title-ref"},"QuantStub")," and a ",(0,o.kt)("span",{class:"title-ref"},"DeQuantStub")," to convert tensors from floating point to quantized type and vice versa:",(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"self.quant = torch.quantization.QuantStub()\nself.dequant = torch.quantization.DeQuantStub()\n")),(0,o.kt)("p",null,"Then in the beginning and the end of the ",(0,o.kt)("span",{class:"title-ref"},"forward")," method of the model definition, call"),(0,o.kt)("span",{class:"title-ref"},"x = self.quant(x)")," and ",(0,o.kt)("span",{class:"title-ref"},"x = self.dequant(x)"),".",(0,o.kt)("p",null,"To do a quantization aware training, use the following code snippet:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre"},"model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\nmodel_qat = torch.quantization.prepare_qat(model, inplace=False)\n# quantization aware training goes here\nmodel_qat = torch.quantization.convert(model_qat.eval(), inplace=False)\n")),(0,o.kt)("p",null,"For more detailed examples of the quantization aware training, see\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/master/quantization.html#quantization-aware-training"},"here"),"\nand\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#quantization-aware-training"},"here"),"."),(0,o.kt)("p",null,"A pre-trained quantized model can also be used for quantized aware\ntransfer learning, using the same ",(0,o.kt)("span",{class:"title-ref"},"quant"),"\nand ",(0,o.kt)("span",{class:"title-ref"},"dequant")," calls shown above. See\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html#part-1-training-a-custom-classifier-based-on-a-quantized-feature-extractor"},"here"),"\nfor a complete example."),(0,o.kt)("p",null,"After a quantized model is generated using one of the steps above,\nbefore the model can be used to run on mobile devices, it needs to be\nfurther converted to the ",(0,o.kt)("span",{class:"title-ref"},"TorchScript"),"\nformat and then optimized for mobile apps. See the ",(0,o.kt)("a",{parentName:"p",href:"script_optimized.html"},"Script and Optimize\nfor Mobile recipe")," for details."),(0,o.kt)("h2",{id:"learn-more"},"Learn More"),(0,o.kt)("p",null,"For more info on the different workflows of quantization, see\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/quantization.html#quantization-workflows"},"here"),"\nand\n",(0,o.kt)("a",{parentName:"p",href:"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/#post-training-static-quantization"},"here"),"."))}d.isMDXComponent=!0}}]);