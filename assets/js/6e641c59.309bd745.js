(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[405],{3905:function(e,t,o){"use strict";o.d(t,{Zo:function(){return s},kt:function(){return m}});var r=o(7294);function i(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function n(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,r)}return o}function a(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?n(Object(o),!0).forEach((function(t){i(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):n(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function d(e,t){if(null==e)return{};var o,r,i=function(e,t){if(null==e)return{};var o,r,i={},n=Object.keys(e);for(r=0;r<n.length;r++)o=n[r],t.indexOf(o)>=0||(i[o]=e[o]);return i}(e,t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)o=n[r],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(i[o]=e[o])}return i}var p=r.createContext({}),l=function(e){var t=r.useContext(p),o=t;return e&&(o="function"==typeof e?e(t):a(a({},t),e)),o},s=function(e){var t=l(e.components);return r.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var o=e.components,i=e.mdxType,n=e.originalType,p=e.parentName,s=d(e,["components","mdxType","originalType","parentName"]),h=l(o),m=i,u=h["".concat(p,".").concat(m)]||h[m]||c[m]||n;return o?r.createElement(u,a(a({ref:t},s),{},{components:o})):r.createElement(u,a({ref:t},s))}));function m(e,t){var o=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var n=o.length,a=new Array(n);a[0]=h;var d={};for(var p in t)hasOwnProperty.call(t,p)&&(d[p]=t[p]);d.originalType=e,d.mdxType="string"==typeof e?e:i,a[1]=d;for(var l=2;l<n;l++)a[l]=o[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,o)}h.displayName="MDXCreateElement"},9586:function(e,t,o){"use strict";o.r(t),o.d(t,{frontMatter:function(){return d},contentTitle:function(){return p},metadata:function(){return l},toc:function(){return s},default:function(){return h}});var r=o(2122),i=o(9756),n=(o(7294),o(3905)),a=["components"],d={id:"android",sidebar_position:11},p="Android",l={unversionedId:"android",id:"android",isDocsHomePage:!1,title:"Android",description:"(This file will be deleted)",source:"@site/docs/android.md",sourceDirName:".",slug:"/android",permalink:"/mobile-ds/docs/android",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/docs/android.md",version:"current",sidebarPosition:11,frontMatter:{id:"android",sidebar_position:11},sidebar:"tutorialSidebar",previous:{title:"Getting Started",permalink:"/mobile-ds/docs/videos/overview"},next:{title:"iOS",permalink:"/mobile-ds/docs/ios"}},s=[{value:"Quickstart with a HelloWorld Example",id:"quickstart-with-a-helloworld-example",children:[]},{value:"PyTorch Demo Application",id:"pytorch-demo-application",children:[]},{value:"More PyTorch Android Demo Apps",id:"more-pytorch-android-demo-apps",children:[{value:"D2go",id:"d2go",children:[]},{value:"Image Segmentation",id:"image-segmentation",children:[]},{value:"Object Detection",id:"object-detection",children:[]},{value:"Neural Machine Translation",id:"neural-machine-translation",children:[]},{value:"Question Answering",id:"question-answering",children:[]},{value:"Vision Transformer",id:"vision-transformer",children:[]},{value:"Speech recognition",id:"speech-recognition",children:[]},{value:"Video Classification",id:"video-classification",children:[]}]},{value:"PyTorch Android Tutorial and Recipes",id:"pytorch-android-tutorial-and-recipes",children:[{value:"Image Segmentation DeepLabV3 on Android",id:"image-segmentation-deeplabv3-on-android",children:[]},{value:"PyTorch Mobile Performance Recipes",id:"pytorch-mobile-performance-recipes",children:[]},{value:"Making Android Native Application That Uses PyTorch Android Prebuilt Libraries",id:"making-android-native-application-that-uses-pytorch-android-prebuilt-libraries",children:[]},{value:"Fuse Modules recipe",id:"fuse-modules-recipe",children:[]},{value:"Quantization for Mobile Recipe",id:"quantization-for-mobile-recipe",children:[]},{value:"Script and Optimize for Mobile",id:"script-and-optimize-for-mobile",children:[]},{value:"Model Preparation for Android Recipe",id:"model-preparation-for-android-recipe",children:[]}]},{value:"Using the PyTorch Android Libraries Built from Source or Nightly",id:"using-the-pytorch-android-libraries-built-from-source-or-nightly",children:[]},{value:"Using the Nightly PyTorch Android Libraries",id:"using-the-nightly-pytorch-android-libraries",children:[]},{value:"Custom Build",id:"custom-build",children:[]},{value:"Use PyTorch JIT interpreter",id:"use-pytorch-jit-interpreter",children:[]},{value:"Android Tutorials",id:"android-tutorials",children:[]},{value:"API Docs",id:"api-docs",children:[]}],c={toc:s};function h(e){var t=e.components,o=(0,i.Z)(e,a);return(0,n.kt)("wrapper",(0,r.Z)({},c,o,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"android"},"Android"),(0,n.kt)("p",null,"(This file will be deleted)"),(0,n.kt)("h2",{id:"quickstart-with-a-helloworld-example"},"Quickstart with a HelloWorld Example"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/HelloWorldApp"},"HelloWorld")," is a simple image classification application that demonstrates how to use PyTorch Android API.\nThis application runs TorchScript serialized TorchVision pretrained resnet18 model on static image which is packaged inside the app as android asset."),(0,n.kt)("h4",{id:"1-model-preparation"},"1. Model Preparation"),(0,n.kt)("p",null,"Let\u2019s start with model preparation. If you are familiar with PyTorch, you probably should already know how to train and save your model. In case you don\u2019t, we are going to use a pre-trained image classification model (",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/hub/pytorch_vision_mobilenet_v2/"},"MobileNetV2"),").\nTo install it, run the command below:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"pip install torchvision\n")),(0,n.kt)("p",null,"To serialize the model you can use python ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/HelloWorldApp/trace_model.py"},"script")," in the root folder of HelloWorld app:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'import torch\nimport torchvision\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.eval()\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model, example)\ntraced_script_module_optimized = optimize_for_mobile(traced_script_module)\ntraced_script_module_optimized._save_for_lite_interpreter("app/src/main/assets/model.ptl")\n\n')),(0,n.kt)("p",null,"If everything works well, we should have our model - ",(0,n.kt)("inlineCode",{parentName:"p"},"model.ptl")," generated in the assets folder of android application.\nThat will be packaged inside android application as ",(0,n.kt)("inlineCode",{parentName:"p"},"asset")," and can be used on the device."),(0,n.kt)("p",null,"More details about TorchScript you can find in ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/jit.html"},"tutorials on pytorch.org")),(0,n.kt)("h4",{id:"2-cloning-from-github"},"2. Cloning from github"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"git clone https://github.com/pytorch/android-demo-app.git\ncd HelloWorldApp\n")),(0,n.kt)("p",null,"If ",(0,n.kt)("a",{parentName:"p",href:"https://developer.android.com/studio/index.html#command-tools"},"Android SDK")," and ",(0,n.kt)("a",{parentName:"p",href:"https://developer.android.com/ndk/downloads"},"Android NDK")," are already installed you can install this application to the connected android device or emulator with:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"./gradlew installDebug\n")),(0,n.kt)("p",null,"We recommend you to open this project in ",(0,n.kt)("a",{parentName:"p",href:"https://developer.android.com/studio"},"Android Studio 3.5.1+"),". At the moment PyTorch Android and demo applications use ",(0,n.kt)("a",{parentName:"p",href:"https://developer.android.com/studio/releases/gradle-plugin#3-5-0"},"android gradle plugin of version 3.5.0"),", which is supported only by Android Studio version 3.5.1 and higher.\nUsing Android Studio you will be able to install Android NDK and Android SDK with Android Studio UI."),(0,n.kt)("h4",{id:"3-gradle-dependencies"},"3. Gradle dependencies"),(0,n.kt)("p",null,"Pytorch android is added to the HelloWorld as ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/HelloWorldApp/app/build.gradle#L28-L29"},"gradle dependencies")," in build.gradle:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"repositories {\n    jcenter()\n}\n\ndependencies {\n    implementation 'org.pytorch:pytorch_android_lite:1.9.0'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.9.0'\n}\n")),(0,n.kt)("p",null,"Where ",(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch:pytorch_android")," is the main dependency with PyTorch Android API, including libtorch native library for all 4 android abis (armeabi-v7a, arm64-v8a, x86, x86_64).\nFurther in this doc you can find how to rebuild it only for specific list of android abis."),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch:pytorch_android_torchvision")," - additional library with utility functions for converting ",(0,n.kt)("inlineCode",{parentName:"p"},"android.media.Image")," and ",(0,n.kt)("inlineCode",{parentName:"p"},"android.graphics.Bitmap")," to tensors."),(0,n.kt)("h4",{id:"4-reading-image-from-android-asset"},"4. Reading image from Android Asset"),(0,n.kt)("p",null,"All the logic happens in ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/HelloWorldApp/app/src/main/java/org/pytorch/helloworld/MainActivity.java#L31-L69"},(0,n.kt)("inlineCode",{parentName:"a"},"org.pytorch.helloworld.MainActivity")),".\nAs a first step we read ",(0,n.kt)("inlineCode",{parentName:"p"},"image.jpg")," to ",(0,n.kt)("inlineCode",{parentName:"p"},"android.graphics.Bitmap")," using the standard Android API."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'Bitmap bitmap = BitmapFactory.decodeStream(getAssets().open("image.jpg"));\n')),(0,n.kt)("h4",{id:"5-loading-mobile-module"},"5. Loading Mobile Module"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'Module module = Module.load(assetFilePath(this, "model.ptl"));\n')),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch.Module")," represents ",(0,n.kt)("inlineCode",{parentName:"p"},"torch::jit::mobile::Module")," that can be loaded with ",(0,n.kt)("inlineCode",{parentName:"p"},"load")," method specifying file path to the serialized to file model."),(0,n.kt)("h4",{id:"6-preparing-input"},"6. Preparing Input"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,\n    TensorImageUtils.TORCHVISION_NORM_MEAN_RGB, TensorImageUtils.TORCHVISION_NORM_STD_RGB);\n")),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch.torchvision.TensorImageUtils")," is part of ",(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch:pytorch_android_torchvision")," library.\nThe ",(0,n.kt)("inlineCode",{parentName:"p"},"TensorImageUtils#bitmapToFloat32Tensor")," method creates tensors in the ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/docs/stable/torchvision/models.html"},"torchvision format")," using ",(0,n.kt)("inlineCode",{parentName:"p"},"android.graphics.Bitmap")," as a source."),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.\nThe images have to be loaded in to a range of ",(0,n.kt)("inlineCode",{parentName:"p"},"[0, 1]")," and then normalized using ",(0,n.kt)("inlineCode",{parentName:"p"},"mean = [0.485, 0.456, 0.406]")," and ",(0,n.kt)("inlineCode",{parentName:"p"},"std = [0.229, 0.224, 0.225]"))),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"inputTensor"),"'s shape is ",(0,n.kt)("inlineCode",{parentName:"p"},"1x3xHxW"),", where ",(0,n.kt)("inlineCode",{parentName:"p"},"H")," and ",(0,n.kt)("inlineCode",{parentName:"p"},"W")," are bitmap height and width appropriately."),(0,n.kt)("h4",{id:"7-run-inference"},"7. Run Inference"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Tensor outputTensor = module.forward(IValue.from(inputTensor)).toTensor();\nfloat[] scores = outputTensor.getDataAsFloatArray();\n")),(0,n.kt)("p",null,(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch.Module.forward")," method runs loaded module's ",(0,n.kt)("inlineCode",{parentName:"p"},"forward")," method and gets result as ",(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch.Tensor")," outputTensor with shape ",(0,n.kt)("inlineCode",{parentName:"p"},"1x1000"),"."),(0,n.kt)("h4",{id:"8-processing-results"},"8. Processing results"),(0,n.kt)("p",null,"Its content is retrieved using ",(0,n.kt)("inlineCode",{parentName:"p"},"org.pytorch.Tensor.getDataAsFloatArray()")," method that returns java array of floats with scores for every image net class."),(0,n.kt)("p",null,"After that we just find index with maximum score and retrieve predicted class name from ",(0,n.kt)("inlineCode",{parentName:"p"},"ImageNetClasses.IMAGENET_CLASSES")," array that contains all ImageNet classes."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"float maxScore = -Float.MAX_VALUE;\nint maxScoreIdx = -1;\nfor (int i = 0; i < scores.length; i++) {\n  if (scores[i] > maxScore) {\n    maxScore = scores[i];\n    maxScoreIdx = i;\n  }\n}\nString className = ImageNetClasses.IMAGENET_CLASSES[maxScoreIdx];\n")),(0,n.kt)("p",null,"In the following sections you can find detailed explanations of PyTorch Android API, code walk through for a bigger ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp"},"demo application"),",\nimplementation details of the API, how to customize and build it from source."),(0,n.kt)("h2",{id:"pytorch-demo-application"},"PyTorch Demo Application"),(0,n.kt)("p",null,"We have also created another more complex PyTorch Android demo application that does image classification from camera output and text classification in the ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/PyTorchDemoApp"},"same github repo"),"."),(0,n.kt)("p",null,"To get device camera output it uses ",(0,n.kt)("a",{parentName:"p",href:"https://developer.android.com/training/camerax"},"Android CameraX API"),".\nAll the logic that works with CameraX is separated to ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/PyTorchDemoApp/app/src/main/java/org/pytorch/demo/vision/AbstractCameraXActivity.java"},(0,n.kt)("inlineCode",{parentName:"a"},"org.pytorch.demo.vision.AbstractCameraXActivity"))," class."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"void setupCameraX() {\n    final PreviewConfig previewConfig = new PreviewConfig.Builder().build();\n    final Preview preview = new Preview(previewConfig);\n    preview.setOnPreviewOutputUpdateListener(output -> mTextureView.setSurfaceTexture(output.getSurfaceTexture()));\n\n    final ImageAnalysisConfig imageAnalysisConfig =\n        new ImageAnalysisConfig.Builder()\n            .setTargetResolution(new Size(224, 224))\n            .setCallbackHandler(mBackgroundHandler)\n            .setImageReaderMode(ImageAnalysis.ImageReaderMode.ACQUIRE_LATEST_IMAGE)\n            .build();\n    final ImageAnalysis imageAnalysis = new ImageAnalysis(imageAnalysisConfig);\n    imageAnalysis.setAnalyzer(\n        (image, rotationDegrees) -> {\n          analyzeImage(image, rotationDegrees);\n        });\n\n    CameraX.bindToLifecycle(this, preview, imageAnalysis);\n  }\n\n  void analyzeImage(android.media.Image, int rotationDegrees)\n")),(0,n.kt)("p",null,"Where the ",(0,n.kt)("inlineCode",{parentName:"p"},"analyzeImage")," method process the camera output, ",(0,n.kt)("inlineCode",{parentName:"p"},"android.media.Image"),"."),(0,n.kt)("p",null,"It uses the aforementioned ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/blob/master/android/pytorch_android_torchvision/src/main/java/org/pytorch/torchvision/TensorImageUtils.java#L90"},(0,n.kt)("inlineCode",{parentName:"a"},"TensorImageUtils.imageYUV420CenterCropToFloat32Tensor"))," method to convert ",(0,n.kt)("inlineCode",{parentName:"p"},"android.media.Image")," in ",(0,n.kt)("inlineCode",{parentName:"p"},"YUV420")," format to input tensor."),(0,n.kt)("p",null,"After getting predicted scores from the model it finds top K classes with the highest scores and shows on the UI."),(0,n.kt)("h4",{id:"language-processing-example"},"Language Processing Example"),(0,n.kt)("p",null,"Another example is natural language processing, based on an LSTM model, trained on a reddit comments dataset.\nThe logic happens in ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/PyTorchDemoApp/app/src/main/java/org/pytorch/demo/nlp/TextClassificationActivity.java"},(0,n.kt)("inlineCode",{parentName:"a"},"TextClassificattionActivity")),"."),(0,n.kt)("p",null,"Result class names are packaged inside the TorchScript model and initialized just after initial module initialization.\nThe module has a ",(0,n.kt)("inlineCode",{parentName:"p"},"get_classes")," method that returns ",(0,n.kt)("inlineCode",{parentName:"p"},"List[str]"),", which can be called using method ",(0,n.kt)("inlineCode",{parentName:"p"},"Module.runMethod(methodName)"),":"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'    mModule = Module.load(moduleFileAbsoluteFilePath);\n    IValue getClassesOutput = mModule.runMethod("get_classes");\n')),(0,n.kt)("p",null,"The returned ",(0,n.kt)("inlineCode",{parentName:"p"},"IValue")," can be converted to java array of ",(0,n.kt)("inlineCode",{parentName:"p"},"IValue")," using ",(0,n.kt)("inlineCode",{parentName:"p"},"IValue.toList()")," and processed to an array of strings using ",(0,n.kt)("inlineCode",{parentName:"p"},"IValue.toStr()"),":"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"    IValue[] classesListIValue = getClassesOutput.toList();\n    String[] moduleClasses = new String[classesListIValue.length];\n    int i = 0;\n    for (IValue iv : classesListIValue) {\n      moduleClasses[i++] = iv.toStr();\n    }\n")),(0,n.kt)("p",null,"Entered text is converted to java array of bytes with ",(0,n.kt)("inlineCode",{parentName:"p"},"UTF-8")," encoding. ",(0,n.kt)("inlineCode",{parentName:"p"},"Tensor.fromBlobUnsigned")," creates tensor of ",(0,n.kt)("inlineCode",{parentName:"p"},"dtype=uint8")," from that array of bytes."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'    byte[] bytes = text.getBytes(Charset.forName("UTF-8"));\n    final long[] shape = new long[]{1, bytes.length};\n    final Tensor inputTensor = Tensor.fromBlobUnsigned(bytes, shape);\n')),(0,n.kt)("p",null,"Running inference of the model is similar to previous examples:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Tensor outputTensor = mModule.forward(IValue.from(inputTensor)).toTensor()\n")),(0,n.kt)("p",null,"After that, the code processes the output, finding classes with the highest scores."),(0,n.kt)("h2",{id:"more-pytorch-android-demo-apps"},"More PyTorch Android Demo Apps"),(0,n.kt)("h3",{id:"d2go"},"D2go"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/D2Go"},"D2Go")," demonstrates a Python script that creates the much lighter and much faster Facebook ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/d2go"},"D2Go")," model that is powered by PyTorch 1.8, torchvision 0.9, and Detectron2 with built-in SOTA networks for mobile, and an Android app that uses it to detect objects from pictures in your photos, taken with camera, or with live camera. This demo app also shows how to use the native pre-built torchvision-ops library."),(0,n.kt)("h3",{id:"image-segmentation"},"Image Segmentation"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/ImageSegmentation"},"Image Segmentation")," demonstrates a Python script that converts the PyTorch ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/"},"DeepLabV3")," model and an Android app that uses the model to segment images."),(0,n.kt)("h3",{id:"object-detection"},"Object Detection"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/ObjectDetection"},"Object Detection")," demonstrates how to convert the popular ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/hub/ultralytics_yolov5/"},"YOLOv5")," model and use it in an Android app that detects objects from pictures in your photos, taken with camera, or with live camera."),(0,n.kt)("h3",{id:"neural-machine-translation"},"Neural Machine Translation"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/Seq2SeqNMT"},"Neural Machine Translation")," demonstrates how to convert a sequence-to-sequence neural machine translation model trained with the code in the ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"},"PyTorch NMT tutorial")," and use the model in an Android app to do French-English translation."),(0,n.kt)("h3",{id:"question-answering"},"Question Answering"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/QuestionAnswering"},"Question Answering")," demonstrates how to convert a powerful transformer QA model and use the model in an Android app to answer questions about PyTorch Mobile and more."),(0,n.kt)("h3",{id:"vision-transformer"},"Vision Transformer"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/ViT4MNIST"},"Vision Transformer")," demonstrates how to use Facebook's latest Vision Transformer ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/deit"},"DeiT")," model to do image classification, and how convert another Vision Transformer model and use it in an Android app to perform handwritten digit recognition."),(0,n.kt)("h3",{id:"speech-recognition"},"Speech recognition"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/SpeechRecognition"},"Speech Recognition")," demonstrates how to convert Facebook AI's wav2vec 2.0, one of the leading models in speech recognition, to TorchScript and how to use the scripted model in an Android app to perform speech recognition."),(0,n.kt)("h3",{id:"video-classification"},"Video Classification"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/tree/master/TorchVideo"},"TorchVideo")," demonstrates how to use a pre-trained video classification model, available at the newly released ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/pytorchvideo"},"PyTorchVideo"),", on Android to see video classification results, updated per second while the video plays, on tested videos, videos from the Photos library, or even real-time videos."),(0,n.kt)("h2",{id:"pytorch-android-tutorial-and-recipes"},"PyTorch Android Tutorial and Recipes"),(0,n.kt)("h3",{id:"image-segmentation-deeplabv3-on-android"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html"},"Image Segmentation DeepLabV3 on Android")),(0,n.kt)("p",null,"A comprehensive step-by-step tutorial on how to prepare and run the PyTorch DeepLabV3 image segmentation model on Android."),(0,n.kt)("h3",{id:"pytorch-mobile-performance-recipes"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/mobile_perf.html"},"PyTorch Mobile Performance Recipes")),(0,n.kt)("p",null,"List of recipes for performance optimizations for using PyTorch on Mobile."),(0,n.kt)("h3",{id:"making-android-native-application-that-uses-pytorch-android-prebuilt-libraries"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/android_native_app_with_custom_op.html"},"Making Android Native Application That Uses PyTorch Android Prebuilt Libraries")),(0,n.kt)("p",null,"Learn how to make Android application from the scratch that uses LibTorch C++ API and uses TorchScript model with custom C++ operator."),(0,n.kt)("h3",{id:"fuse-modules-recipe"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/fuse.html"},"Fuse Modules recipe")),(0,n.kt)("p",null,"Learn how to fuse a list of PyTorch modules into a single module to reduce the model size before quantization."),(0,n.kt)("h3",{id:"quantization-for-mobile-recipe"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/quantization.html"},"Quantization for Mobile Recipe")),(0,n.kt)("p",null,"Learn how to reduce the model size and make it run faster without losing much on accuracy."),(0,n.kt)("h3",{id:"script-and-optimize-for-mobile"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/script_optimized.html"},"Script and Optimize for Mobile")),(0,n.kt)("p",null,"Learn how to convert the model to TorchScipt and (optional) optimize it for mobile apps."),(0,n.kt)("h3",{id:"model-preparation-for-android-recipe"},(0,n.kt)("a",{parentName:"h3",href:"https://pytorch.org/tutorials/recipes/model_preparation_android.html"},"Model Preparation for Android Recipe")),(0,n.kt)("p",null,"Learn how to add the model in an Android project and use the PyTorch library for Android."),(0,n.kt)("h2",{id:"using-the-pytorch-android-libraries-built-from-source-or-nightly"},"Using the PyTorch Android Libraries Built from Source or Nightly"),(0,n.kt)("p",null,"First add the two aar files built above, or downloaded from the nightly built PyTorch Android repos at ",(0,n.kt)("a",{parentName:"p",href:"https://oss.sonatype.org/#nexus-search;quick~pytorch_android"},"here")," and ",(0,n.kt)("a",{parentName:"p",href:"https://oss.sonatype.org/#nexus-search;quick~torchvision_android"},"here"),", to the Android project's ",(0,n.kt)("inlineCode",{parentName:"p"},"lib")," folder, then add in the project's app ",(0,n.kt)("inlineCode",{parentName:"p"},"build.gradle")," file:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"allprojects {\n    repositories {\n        flatDir {\n            dirs 'libs'\n        }\n    }\n}\n\ndependencies {\n\n    // if using the libraries built from source\n    implementation(name:'pytorch_android-release', ext:'aar')\n    implementation(name:'pytorch_android_torchvision-release', ext:'aar')\n\n    // if using the nightly built libraries downloaded above, for example the 1.8.0-snapshot on Jan. 21, 2021\n    // implementation(name:'pytorch_android-1.8.0-20210121.092759-172', ext:'aar')\n    // implementation(name:'pytorch_android_torchvision-1.8.0-20210121.092817-173', ext:'aar')\n\n    ...\n    implementation 'com.android.support:appcompat-v7:28.0.0'\n    implementation 'com.facebook.fbjni:fbjni-java-only:0.0.3'\n}\n")),(0,n.kt)("p",null,"Also we have to add all transitive dependencies of our aars. As ",(0,n.kt)("inlineCode",{parentName:"p"},"pytorch_android")," depends on ",(0,n.kt)("inlineCode",{parentName:"p"},"com.android.support:appcompat-v7:28.0.0")," or ",(0,n.kt)("inlineCode",{parentName:"p"},"androidx.appcompat:appcompat:1.2.0"),", we need to one of them. (In case of using maven dependencies they are added automatically from ",(0,n.kt)("inlineCode",{parentName:"p"},"pom.xml"),")."),(0,n.kt)("h2",{id:"using-the-nightly-pytorch-android-libraries"},"Using the Nightly PyTorch Android Libraries"),(0,n.kt)("p",null,"Other than using the aar files built from source or downloaded from the links in the previous section, you can also use the nightly built Android PyTorch and TorchVision libraries by adding in your app ",(0,n.kt)("inlineCode",{parentName:"p"},"build.gradle")," file the maven url and the nightly libraries implementation as follows:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"repositories {\n    maven {\n        url \"https://oss.sonatype.org/content/repositories/snapshots\"\n    }\n}\n\ndependencies {\n    ...\n    implementation 'org.pytorch:pytorch_android:1.8.0-SNAPSHOT'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.8.0-SNAPSHOT'\n}\n")),(0,n.kt)("p",null,"This is the easiest way to try out the latest PyTorch code and the Android libraries, if you do not need to make any local changes. But be aware you may need to build the model used on mobile in the latest PyTorch - using either the latest PyTorch code or a quick nightly install with commands like ",(0,n.kt)("inlineCode",{parentName:"p"},"pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html")," - to avoid possible model version mismatch errors when running the model on mobile."),(0,n.kt)("h2",{id:"custom-build"},"Custom Build"),(0,n.kt)("p",null,"To reduce the size of binaries you can do custom build of PyTorch Android with only set of operators required by your model.\nThis includes two steps: preparing the list of operators from your model, rebuilding pytorch android with specified list."),(0,n.kt)("p",null,"1","."," Verify your PyTorch version is 1.4.0 or above. You can do that by checking the value of ",(0,n.kt)("inlineCode",{parentName:"p"},"torch.__version__"),"."),(0,n.kt)("p",null,"2","."," Preparation of the list of operators"),(0,n.kt)("p",null,"List of operators of your serialized torchscript model can be prepared in yaml format using python api function ",(0,n.kt)("inlineCode",{parentName:"p"},"torch.jit.export_opnames()"),".\nTo dump the operators in your model, say ",(0,n.kt)("inlineCode",{parentName:"p"},"MobileNetV2"),", run the following lines of Python code:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"# Dump list of operators used by MobileNetV2:\nimport torch, yaml\nmodel = torch.jit.load('MobileNetV2.pt')\nops = torch.jit.export_opnames(model)\nwith open('MobileNetV2.yaml', 'w') as output:\n    yaml.dump(ops, output)\n")),(0,n.kt)("p",null,"3","."," Building PyTorch Android with prepared operators list."),(0,n.kt)("p",null,"To build PyTorch Android with the prepared yaml list of operators, specify it in the environment variable ",(0,n.kt)("inlineCode",{parentName:"p"},"SELECTED_OP_LIST"),". Also in the arguments, specify which Android ABIs it should build; by default it builds all 4 Android ABIs."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"# Build PyTorch Android library customized for MobileNetV2:\nSELECTED_OP_LIST=MobileNetV2.yaml scripts/build_pytorch_android.sh arm64-v8a\n")),(0,n.kt)("p",null,"After successful build you can integrate the result aar files to your android gradle project, following the steps from previous section of this tutorial (Building PyTorch Android from Source)."),(0,n.kt)("h2",{id:"use-pytorch-jit-interpreter"},"Use PyTorch JIT interpreter"),(0,n.kt)("p",null,"PyTorch JIT interpreter is the default interpreter before 1.9 (a version of our PyTorch interpreter that is not as size-efficient). It will still be supported in 1.9, and can be used via ",(0,n.kt)("inlineCode",{parentName:"p"},"build.gradle"),":"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"repositories {\n    jcenter()\n}\n\ndependencies {\n    implementation 'org.pytorch:pytorch_android:1.9.0'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.9.0'\n}\n")),(0,n.kt)("h2",{id:"android-tutorials"},"Android Tutorials"),(0,n.kt)("p",null,"Watch the following ",(0,n.kt)("a",{parentName:"p",href:"https://youtu.be/5Lxuu16_28o"},"video")," as PyTorch Partner Engineer Brad Heintz walks through steps for setting up the PyTorch Runtime for Android projects:"),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://youtu.be/5Lxuu16_28o",title:"PyTorch Mobile Runtime for Android"},(0,n.kt)("img",{parentName:"a",src:"https://i.ytimg.com/vi/O_2KBhkIvnc/maxresdefault.jpg",alt:"PyTorch Mobile Runtime for Android"}),'{:height="75%" width="75%"}')),(0,n.kt)("p",null,"The corresponding code can be found ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/pytorch/workshops/tree/master/PTMobileWalkthruAndroid"},"here"),"."),(0,n.kt)("p",null,"Checkout our ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/recipes/mobile_perf.html"},"Mobile Performance Recipes")," which cover how to optimize your model and check if optimizations helped via benchmarking."),(0,n.kt)("p",null,"In addition, follow this recipe to learn how to ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/recipes/android_native_app_with_custom_op.html"},"make Native Android Application that use PyTorch prebuilt libraries"),"."),(0,n.kt)("h2",{id:"api-docs"},"API Docs"),(0,n.kt)("p",null,"You can find more details about the PyTorch Android API in the ",(0,n.kt)("a",{parentName:"p",href:"https://pytorch.org/javadoc/"},"Javadoc"),"."),(0,n.kt)("script",{"page-id":"android",src:"{{ site.baseurl }}/assets/menu-tab-selection.js"}))}h.isMDXComponent=!0}}]);