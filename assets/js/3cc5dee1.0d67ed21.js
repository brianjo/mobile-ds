(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[232],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return u},kt:function(){return m}});var o=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,r=function(e,t){if(null==e)return{};var n,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var p=o.createContext({}),s=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},u=function(e){var t=s(e.components);return o.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},d=o.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=s(n),m=r,h=d["".concat(p,".").concat(m)]||d[m]||c[m]||i;return n?o.createElement(h,a(a({ref:t},u),{},{components:n})):o.createElement(h,a({ref:t},u))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,a=new Array(i);a[0]=d;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:r,a[1]=l;for(var s=2;s<i;s++)a[s]=n[s];return o.createElement.apply(null,a)}return o.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7065:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return l},contentTitle:function(){return p},metadata:function(){return s},toc:function(){return u},default:function(){return d}});var o=n(2122),r=n(9756),i=(n(7294),n(3905)),a=["components"],l={},p="Pytorch Mobile Performance Recipes",s={unversionedId:"performance/mobile_perf",id:"performance/mobile_perf",isDocsHomePage:!1,title:"Pytorch Mobile Performance Recipes",description:"Introduction",source:"@site/docs/performance/mobile_perf.md",sourceDirName:"performance",slug:"/performance/mobile_perf",permalink:"/mobile-ds/docs/performance/mobile_perf",editUrl:"https://github.com/facebook/docusaurus/edit/master/website/docs/performance/mobile_perf.md",version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Build PyTorch iOS Libraries from Source",permalink:"/mobile-ds/docs/building/iosbuild"},next:{title:"Overview",permalink:"/mobile-ds/docs/tutorials/overview"}},u=[{value:"Introduction",id:"introduction",children:[]},{value:"Model preparation",id:"model-preparation",children:[{value:"Setup",id:"setup",children:[]},{value:"1. Fuse operators using <code>torch.quantization.fuse_modules</code>",id:"1-fuse-operators-using-torchquantizationfuse_modules",children:[]},{value:"2. Quantize your model",id:"2-quantize-your-model",children:[]},{value:"3. Use torch.utils.mobile_optimizer",id:"3-use-torchutilsmobile_optimizer",children:[]},{value:"4. Prefer Using Channels Last Tensor memory format",id:"4-prefer-using-channels-last-tensor-memory-format",children:[]},{value:"5. Android - Reusing tensors for forward",id:"5-android---reusing-tensors-for-forward",children:[]}]},{value:"Benchmarking",id:"benchmarking",children:[{value:"Android - Benchmarking Setup",id:"android---benchmarking-setup",children:[]},{value:"iOS - Benchmarking Setup",id:"ios---benchmarking-setup",children:[]}]}],c={toc:u};function d(e){var t=e.components,n=(0,r.Z)(e,a);return(0,i.kt)("wrapper",(0,o.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"pytorch-mobile-performance-recipes"},"Pytorch Mobile Performance Recipes"),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"Performance (aka latency) is crucial to most, if not all, applications\nand use-cases of ML model inference on mobile devices."),(0,i.kt)("p",null,"Today, PyTorch executes the models on the CPU backend pending\navailability of other hardware backends such as GPU, DSP, and NPU."),(0,i.kt)("p",null,"In this recipe, you will learn:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"How to optimize your model to help decrease execution time (higher\nperformance, lower latency) on the mobile device."),(0,i.kt)("li",{parentName:"ul"},"How to benchmark (to check if optimizations helped your use case).")),(0,i.kt)("h2",{id:"model-preparation"},"Model preparation"),(0,i.kt)("p",null,"We will start with preparing to optimize your model to help decrease\nexecution time (higher performance, lower latency) on the mobile device."),(0,i.kt)("h3",{id:"setup"},"Setup"),(0,i.kt)("p",null,"First we need to installed pytorch using conda or pip with version at\nleast 1.5.0."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"conda install pytorch torchvision -c pytorch\n")),(0,i.kt)("p",null,"or"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"pip install torch torchvision\n")),(0,i.kt)("p",null,"Code your model:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"import torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nclass AnnotatedConvBnReLUModel(torch.nn.Module):\n    def __init__(self):\n        super(AnnotatedConvBnReLUModel, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x.contiguous(memory_format=torch.channels_last)\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\nmodel = AnnotatedConvBnReLUModel()\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"torch.quantization.QuantStub")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.quantization.DeQuantStub()"),"\nare no-op stubs, which will be used for quantization step."),(0,i.kt)("h3",{id:"1-fuse-operators-using-torchquantizationfuse_modules"},"1. Fuse operators using ",(0,i.kt)("inlineCode",{parentName:"h3"},"torch.quantization.fuse_modules")),(0,i.kt)("p",null,"Do not be confused that fuse","_","modules is in the quantization package. It\nworks for all ",(0,i.kt)("inlineCode",{parentName:"p"},"torch.nn.Module"),"."),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"torch.quantization.fuse_modules")," fuses a list of modules into a single\nmodule. It fuses only the following sequence of modules:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Convolution, Batch normalization"),(0,i.kt)("li",{parentName:"ul"},"Convolution, Batch normalization, Relu"),(0,i.kt)("li",{parentName:"ul"},"Convolution, Relu"),(0,i.kt)("li",{parentName:"ul"},"Linear, Relu")),(0,i.kt)("p",null,"This script will fuse Convolution, Batch Normalization and Relu in\npreviously declared model."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']], inplace=True)\n")),(0,i.kt)("h3",{id:"2-quantize-your-model"},"2. Quantize your model"),(0,i.kt)("p",null,"You can find more about PyTorch quantization in ",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/"},"the dedicated\ntutorial"),"."),(0,i.kt)("p",null,"Quantization of the model not only moves computation to int8, but also\nreduces the size of your model on a disk. That size reduction helps to\nreduce disk read operations during the first load of the model and\ndecreases the amount of RAM. Both of those resources can be crucial for\nthe performance of mobile applications. This code does quantization,\nusing stub for model calibration function, you can find more about it\n",(0,i.kt)("a",{parentName:"p",href:"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#post-training-static-quantization"},"here"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\ntorch.quantization.prepare(model, inplace=True)\n# Calibrate your model\ndef calibrate(model, calibration_data):\n    # Your calibration code here\n    return\ncalibrate(model, [])\ntorch.quantization.convert(model, inplace=True)\n")),(0,i.kt)("h3",{id:"3-use-torchutilsmobile_optimizer"},"3. Use torch.utils.mobile","_","optimizer"),(0,i.kt)("p",null,"Torch mobile","_","optimizer package does several optimizations with the\nscripted model, which will help to conv2d and linear operations. It\npre-packs model weights in an optimized format and fuses ops above with\nrelu if it is the next operation."),(0,i.kt)("p",null,"First we script the result model from previous step:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"torchscript_model = torch.jit.script(model)\n")),(0,i.kt)("p",null,"Next we call ",(0,i.kt)("inlineCode",{parentName:"p"},"optimize_for_mobile")," and save model on the disk."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'torchscript_model_optimized = optimize_for_mobile(torchscript_model)\ntorch.jit.save(torchscript_model_optimized, "model.pt")\n')),(0,i.kt)("h3",{id:"4-prefer-using-channels-last-tensor-memory-format"},"4. Prefer Using Channels Last Tensor memory format"),(0,i.kt)("p",null,"Channels Last(NHWC) memory format was introduced in PyTorch 1.4.0. It is\nsupported only for four-dimensional tensors. This memory format gives a\nbetter memory locality for most operators, especially convolution. Our\nmeasurements showed a 3x speedup of MobileNetV2 model compared with the\ndefault Channels First(NCHW) format."),(0,i.kt)("p",null,"At the moment of writing this recipe, PyTorch Android java API does not\nsupport using inputs in Channels Last memory format. But it can be used\non the TorchScript model level, by adding the conversion to it for model\ninputs."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"def forward(self, x):\n    x.contiguous(memory_format=torch.channels_last)\n    ...\n")),(0,i.kt)("p",null,"This conversion is zero cost if your input is already in Channels Last\nmemory format. After it, all operators will work preserving ChannelsLast\nmemory format."),(0,i.kt)("h3",{id:"5-android---reusing-tensors-for-forward"},"5. Android - Reusing tensors for forward"),(0,i.kt)("p",null,"This part of the recipe is Android only."),(0,i.kt)("p",null,"Memory is a critical resource for android performance, especially on old\ndevices. Tensors can need a significant amount of memory. For example,\nstandard computer vision tensor contains 1","*","3","*","224","*","224 elements,\nassuming that data type is float and will need 588Kb of memory."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"FloatBuffer buffer = Tensor.allocateFloatBuffer(1*3*224*224);\nTensor tensor = Tensor.fromBlob(buffer, new long[]{1, 3, 224, 224});\n")),(0,i.kt)("p",null,"Here we allocate native memory as ",(0,i.kt)("inlineCode",{parentName:"p"},"java.nio.FloatBuffer")," and creating\n",(0,i.kt)("inlineCode",{parentName:"p"},"org.pytorch.Tensor")," which storage will be pointing to the memory of the\nallocated buffer."),(0,i.kt)("p",null,"For most of the use cases, we do not do model forward only once,\nrepeating it with some frequency or as fast as possible."),(0,i.kt)("p",null,"If we are doing new memory allocation for every module forward - that\nwill be suboptimal. Instead of this, we can reuse the same memory that\nwe allocated on the previous step, fill it with new data, and run module\nforward again on the same tensor object."),(0,i.kt)("p",null,"You can check how it looks in code in ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pytorch/android-demo-app/blob/master/PyTorchDemoApp/app/src/main/java/org/pytorch/demo/vision/ImageClassificationActivity.java#L174"},"pytorch android application\nexample"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"protected AnalysisResult analyzeImage(ImageProxy image, int rotationDegrees) {\n  if (mModule == null) {\n    mModule = Module.load(moduleFileAbsoluteFilePath);\n    mInputTensorBuffer =\n    Tensor.allocateFloatBuffer(3 * 224 * 224);\n    mInputTensor = Tensor.fromBlob(mInputTensorBuffer, new long[]{1, 3, 224, 224});\n  }\n\n  TensorImageUtils.imageYUV420CenterCropToFloatBuffer(\n      image.getImage(), rotationDegrees,\n      224, 224,\n      TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n      TensorImageUtils.TORCHVISION_NORM_STD_RGB,\n      mInputTensorBuffer, 0);\n\n  Tensor outputTensor = mModule.forward(IValue.from(mInputTensor)).toTensor();\n}\n")),(0,i.kt)("p",null,"Member fields ",(0,i.kt)("inlineCode",{parentName:"p"},"mModule"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"mInputTensorBuffer")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"mInputTensor")," are\ninitialized only once and buffer is refilled using\n",(0,i.kt)("inlineCode",{parentName:"p"},"org.pytorch.torchvision.TensorImageUtils.imageYUV420CenterCropToFloatBuffer"),"."),(0,i.kt)("h2",{id:"benchmarking"},"Benchmarking"),(0,i.kt)("p",null,"The best way to benchmark (to check if optimizations helped your use\ncase) - is to measure your particular use case that you want to\noptimize, as performance behavior can vary in different environments."),(0,i.kt)("p",null,"PyTorch distribution provides a way to benchmark naked binary that runs\nthe model forward, this approach can give more stable measurements\nrather than testing inside the application."),(0,i.kt)("h3",{id:"android---benchmarking-setup"},"Android - Benchmarking Setup"),(0,i.kt)("p",null,"This part of the recipe is Android only."),(0,i.kt)("p",null,"For this you first need to build benchmark binary:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"<from-your-root-pytorch-dir>\nrm -rf build_android\nBUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh -DBUILD_BINARY=ON\n")),(0,i.kt)("p",null,"You should have arm64 binary at:\n",(0,i.kt)("inlineCode",{parentName:"p"},"build_android/bin/speed_benchmark_torch"),". This binary takes\n",(0,i.kt)("inlineCode",{parentName:"p"},"--model=<path-to-model>"),", ",(0,i.kt)("inlineCode",{parentName:"p"},'--input_dim="1,3,224,224"')," as dimension\ninformation for the input and ",(0,i.kt)("inlineCode",{parentName:"p"},'--input_type="float"')," as the type of the\ninput as arguments."),(0,i.kt)("p",null,"Once you have your android device connected, push speedbenchark","_","torch\nbinary and your model to the phone:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"adb push <speedbenchmark-torch> /data/local/tmp\nadb push <path-to-scripted-model> /data/local/tmp\n")),(0,i.kt)("p",null,"Now we are ready to benchmark your model:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'adb shell "/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model.pt" --input_dims="1,3,224,224" --input_type="float"\n----- output -----\nStarting benchmark.\nRunning warmup runs.\nMain runs.\nMain run finished. Microseconds per iter: 121318. Iters per second: 8.24281\n')),(0,i.kt)("h3",{id:"ios---benchmarking-setup"},"iOS - Benchmarking Setup"),(0,i.kt)("p",null,"For iOS, we'll be using our\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/tree/master/ios/TestApp"},"TestApp")," as\nthe benchmarking tool."),(0,i.kt)("p",null,"To begin with, let's apply the ",(0,i.kt)("inlineCode",{parentName:"p"},"optimize_for_mobile")," method to our\npython script located at\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/pytorch/pytorch/blob/master/ios/TestApp/benchmark/trace_model.py"},"TestApp/benchmark/trace","_","model.py"),".\nSimply modify the code as below."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},'import torch\nimport torchvision\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.eval()\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model, example)\ntorchscript_model_optimized = optimize_for_mobile(traced_script_module)\ntorch.jit.save(torchscript_model_optimized, "model.pt")\n')),(0,i.kt)("p",null,"Now let's run ",(0,i.kt)("inlineCode",{parentName:"p"},"python trace_model.py"),". If everything works well, we\nshould be able to generate our optimized model in the benchmark\ndirectory."),(0,i.kt)("p",null,"Next, we're going to build the PyTorch libraries from source."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh\n")),(0,i.kt)("p",null,"Now that we have the optimized model and PyTorch ready, it's time to\ngenerate our XCode project and do benchmarking. To do that, we'll be\nusing a ruby script - ",(0,i.kt)("span",{class:"title-ref"},"setup.rb")," which does\nthe heavy lifting jobs of setting up the XCode project."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"ruby setup.rb\n")),(0,i.kt)("p",null,"Now open the ",(0,i.kt)("span",{class:"title-ref"},"TestApp.xcodeproj")," and plug\nin your iPhone, you're ready to go. Below is an example result from\niPhoneX"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"TestApp[2121:722447] Main runs\nTestApp[2121:722447] Main run finished. Milliseconds per iter: 28.767\nTestApp[2121:722447] Iters per second: : 34.762\nTestApp[2121:722447] Done.\n")))}d.isMDXComponent=!0}}]);